{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXHx4Q5AZFW0eK1tMD8eQD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akanbiabiodun25/Hamoye-166a464ba141f000/blob/main/Hamoye_Stage_D_Quiz(Fine_Tuning).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I9wK-NAjMsI",
        "outputId": "b9559f25-714d-4a54-ed2d-cb8fd3d1f48a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentence pairs in training set: 3668\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "num_train_pairs = len(dataset[\"train\"])\n",
        "\n",
        "print(f\"Number of sentence pairs in training set: {num_train_pairs}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "print(dataset[\"train\"].features[\"label\"].dtype)  # Output might be \"int64\" or \"ClassLabel\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qocx0mnfkACD",
        "outputId": "a53f6ef6-8270-43c3-cba9-83e5633c7430"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the MRPC dataset, specifying the dataset name (\"glue\") and task (\"mrpc\")\n",
        "# Optionally, specify the split (\"train\", \"validation\", or \"test\")\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLmKs-5ylJ6P",
        "outputId": "747e0477-9a0d-4897-ecfb-dc333311ce2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for a pre-trained model (replace \"bert-base-cased\" if desired)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Function to tokenize sentence pairs (without padding/truncation)\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"sentence1\"],\n",
        "        examples[\"sentence2\"],\n",
        "        return_tensors=\"pt\",  # Return tensors for PyTorch (optional)\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=52  # Set the desired maximum length here (optional)\n",
        "    )\n",
        "\n",
        "# Load the MRPC dataset (requires the datasets library)\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
        "\n",
        "# Apply tokenization function to the entire dataset (may take time)\n",
        "processed_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Access the 3rd element (index 2) of the processed 'sentence1'\n",
        "tokenized_sentence1 = processed_dataset[\"input_ids\"][2]\n",
        "\n",
        "# Determine the number of tokens in the 3rd element\n",
        "num_tokens = len(tokenized_sentence1)\n",
        "\n",
        "# Print the number of tokens (may vary depending on the dataset)\n",
        "print(f\"Number of tokens: {num_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoZJPzoklkJ2",
        "outputId": "6fb7afee-172a-4299-cd27-453551a4d0f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# Check if 'test' split exists (using the loaded dataset variable)\n",
        "if 'test' in dataset:\n",
        "    first_sentence = dataset['test'][0]['sentence1']  # Access first sentence\n",
        "    print(f\"Content of the first sentence in 'test' split:\")\n",
        "    print(first_sentence)\n",
        "else:\n",
        "    print(\"'test' split not found in the loaded dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH10dkZrkqMl",
        "outputId": "9ac7ae3f-0cc7-4cd4-9397-8db45c0148c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of the first sentence in 'test' split:\n",
            "PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
        "\n",
        "# Access the third data sample (index 2)\n",
        "third_sample = dataset[2]\n",
        "\n",
        "# Extract the second sentence\n",
        "second_sentence = third_sample[\"sentence2\"]\n",
        "\n",
        "# Print the second sentence\n",
        "print(f\"Content of the second sentence in the third data sample (train split):\")\n",
        "print(second_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRkhJMOFnpey",
        "outputId": "8a4b091a-70cc-4428-c815-51781e08aa8c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of the second sentence in the third data sample (train split):\n",
            "On June 10 , the ship 's owners had published an advertisement on the Internet , offering the explosives for sale .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load tokenizer (replace \"bert-base-cased\" if desired)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Function to tokenize sentence pairs (without padding/truncation)\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"sentence1\"],\n",
        "      examples[\"sentence2\"],\n",
        "      return_tensors=\"pt\",  # Return tensors for PyTorch (optional)\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=52  # Set the desired maximum length here (optional)\n",
        "  )\n",
        "\n",
        "# Load the MRPC dataset (train split)\n",
        "dataset = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
        "\n",
        "# Apply tokenization function to the entire dataset (may take time)\n",
        "processed_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Access the 3rd element (index 2) of the processed 'sentence1'\n",
        "tokenized_sentence1 = processed_dataset[\"input_ids\"][2]\n",
        "\n",
        "# Determine the number of tokens in the 3rd element\n",
        "num_tokens = len(tokenized_sentence1)\n",
        "\n",
        "# Print the number of tokens\n",
        "print(f\"Number of tokens in the 3rd content of tokenized sentence1: {num_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKq6asXZoXyf",
        "outputId": "f193f138-2768-4719-d9b4-81e01748464f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the 3rd content of tokenized sentence1: 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# Access the training data\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Get the first data sample from the training data\n",
        "first_sample = train_data[0]\n",
        "\n",
        "# Access specific features of the first sample (e.g., sentence1, sentence2, label)\n",
        "sentence1 = first_sample[\"sentence1\"]\n",
        "sentence2 = first_sample[\"sentence2\"]\n",
        "label = first_sample[\"label\"]  # Assuming labels are present\n",
        "\n",
        "# Iterate through all samples in the training data\n",
        "for sample in train_data:\n",
        "  # Process each sample here\n",
        "  pass"
      ],
      "metadata": {
        "id": "-a-iJFXbqIot"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "# Access the training data\n",
        "train_data = dataset[\"train\"]\n",
        "\n",
        "# Get the first data sample from the training data\n",
        "first_sample = train_data[0]\n",
        "\n",
        "# Access specific features of the first sample (e.g., sentence1, sentence2, label)\n",
        "sentence1 = first_sample[\"sentence1\"]\n",
        "sentence2 = first_sample[\"sentence2\"]\n",
        "label = first_sample[\"label\"]  # Assuming labels are present\n",
        "\n",
        "# Print the first sentence and its label\n",
        "print(f\"Sentence 1: {sentence1}\")\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxSv7p3sqW-5",
        "outputId": "0891f364-f213-480b-ee1d-8d96595cc23c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mrpc\", split=\"test\")  # Specify the test split\n",
        "\n",
        "# Check if 'test' split exists\n",
        "if len(dataset) > 0:  # Check if there are any examples in the test split\n",
        "  first_sample = dataset[0]\n",
        "  first_sentence = first_sample[\"sentence1\"]\n",
        "  print(f\"Content of the first sentence in 'test' split:\")\n",
        "  print(first_sentence)\n",
        "else:\n",
        "  print(\"'test' split appears to be empty or not found in the loaded dataset.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTQjfUsTrGVI",
        "outputId": "d8dcb21b-7dd5-4bbc-9a88-2dd36f20ccf3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of the first sentence in 'test' split:\n",
            "PCCW 's chief operating officer , Mike Butcher , and Alex Arena , the chief financial officer , will report directly to Mr So .\n"
          ]
        }
      ]
    }
  ]
}